{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning for Natural Language Processing\n",
    "\n",
    "\n",
    " * Simple text representations, bag of words\n",
    " * Word embedding and... not just another word2vec this time\n",
    " * rnn for text\n",
    " * Aggregating several data sources \"the hard way\"\n",
    " * Solving ~somewhat~ real ML problem with ~almost~ end-to-end deep learning\n",
    " \n",
    "\n",
    "Special thanks to Irina Golzmann for help with technical part, task prepared by Александр Панин, jheuristic@yandex-team.ru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK\n",
    "\n",
    "You will require nltk v3.2 to solve this assignment\n",
    "\n",
    "__It is really important that the version is 3.2, otherwize russian tokenizer might not work__\n",
    "\n",
    "Install/update\n",
    "* `sudo pip install --upgrade nltk==3.2`\n",
    "* If you don't remember when was the last pip upgrade, `sudo pip install --upgrade pip`\n",
    "\n",
    "If for some reason you can't or won't switch to nltk v3.2, just make sure that russian words are tokenized properly with RegeExpTokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For students with low-RAM machines\n",
    " * This assignment can be accomplished with even the low-tier hardware (<= 4Gb RAM) \n",
    " * If that is the case, turn flag \"low_RAM_mode\" below to True\n",
    " * If you have around 8GB memory, it is unlikely that you will feel constrained by memory.\n",
    " * In case you are using a PC from last millenia, consider setting very_low_RAM=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "low_RAM_mode = False\n",
    "very_low_RAM = False  #If you have <3GB RAM, set BOTH to true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Ex-kaggle-competition on prohibited content detection\n",
    "\n",
    "There goes the description - https://www.kaggle.com/c/avito-prohibited-content\n",
    "\n",
    "\n",
    "### Download\n",
    "High-RAM mode,\n",
    " * Download avito_train.tsv from competition data files\n",
    "Low-RAM-mode,\n",
    " * Download downsampled dataset from here\n",
    "     * archive https://yadi.sk/d/l0p4lameqw3W8\n",
    "     * raw https://yadi.sk/d/I1v7mZ6Sqw2WK (in case you feel masochistic)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# What's inside\n",
    "Different kinds of features:\n",
    "* 2 text fields - title and description\n",
    "* Special features - price, number of e-mails, phones, etc\n",
    "* Category and subcategory - unsurprisingly, categorical features\n",
    "* Attributes - more factors\n",
    "\n",
    "Only 1 binary target whether or not such advertisement contains prohibited materials\n",
    "* criminal, misleading, human reproduction-related, etc\n",
    "* diving into the data may result in prolonged sleep disorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not low_RAM_mode:\n",
    "    # a lot of ram\n",
    "    df = pd.read_csv(\"avito_train_1kk.tsv\",sep='\\t')\n",
    "else:\n",
    "    #aroung 4GB ram\n",
    "    df = pd.read_csv(\"avito_train_1kk.tsv\",sep='\\t')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1204949, 13) 0.228222107326\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemid</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>attrs</th>\n",
       "      <th>price</th>\n",
       "      <th>is_proved</th>\n",
       "      <th>is_blocked</th>\n",
       "      <th>phones_cnt</th>\n",
       "      <th>emails_cnt</th>\n",
       "      <th>urls_cnt</th>\n",
       "      <th>close_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000010</td>\n",
       "      <td>Транспорт</td>\n",
       "      <td>Автомобили с пробегом</td>\n",
       "      <td>Toyota Sera, 1991</td>\n",
       "      <td>Новая оригинальная линзованая оптика на ксенон...</td>\n",
       "      <td>{\"Год выпуска\":\"1991\", \"Тип кузова\":\"Купе\", \"П...</td>\n",
       "      <td>150000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000094</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Одежда, обувь, аксессуары</td>\n",
       "      <td>Костюм Steilmann</td>\n",
       "      <td>Юбка и топ из панбархата. Под топ  трикотажная...</td>\n",
       "      <td>{\"Вид одежды\":\"Женская одежда\", \"Предмет одежд...</td>\n",
       "      <td>1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000299</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Детская одежда и обувь</td>\n",
       "      <td>Костюм Didriksons Boardman, размер 100, краги,...</td>\n",
       "      <td>Костюм Didriksons Boardman, в отличном состоян...</td>\n",
       "      <td>{\"Вид одежды\":\"Для мальчиков\", \"Предмет одежды...</td>\n",
       "      <td>3000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000309</td>\n",
       "      <td>Недвижимость</td>\n",
       "      <td>Квартиры</td>\n",
       "      <td>1-к квартира, 44 м², 9/20 эт.</td>\n",
       "      <td>В кирпичном пан.-м доме, продается одноком.-ая...</td>\n",
       "      <td>{\"Тип объявления\":\"Продам\", \"Количество комнат...</td>\n",
       "      <td>2642020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000317</td>\n",
       "      <td>Услуги</td>\n",
       "      <td>Предложения услуг</td>\n",
       "      <td>Поездки на таможню, печать в паспорте</td>\n",
       "      <td>Поездки на таможню гражданам СНГ для пересечен...</td>\n",
       "      <td>{\"Вид услуги\":\"Деловые услуги\", \"Тип услуги\":\"...</td>\n",
       "      <td>1500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     itemid      category                subcategory  \\\n",
       "0  10000010     Транспорт      Автомобили с пробегом   \n",
       "1  10000094   Личные вещи  Одежда, обувь, аксессуары   \n",
       "2  10000299   Личные вещи     Детская одежда и обувь   \n",
       "3  10000309  Недвижимость                   Квартиры   \n",
       "4  10000317        Услуги          Предложения услуг   \n",
       "\n",
       "                                               title  \\\n",
       "0                                  Toyota Sera, 1991   \n",
       "1                                   Костюм Steilmann   \n",
       "2  Костюм Didriksons Boardman, размер 100, краги,...   \n",
       "3                      1-к квартира, 44 м², 9/20 эт.   \n",
       "4              Поездки на таможню, печать в паспорте   \n",
       "\n",
       "                                         description  \\\n",
       "0  Новая оригинальная линзованая оптика на ксенон...   \n",
       "1  Юбка и топ из панбархата. Под топ  трикотажная...   \n",
       "2  Костюм Didriksons Boardman, в отличном состоян...   \n",
       "3  В кирпичном пан.-м доме, продается одноком.-ая...   \n",
       "4  Поездки на таможню гражданам СНГ для пересечен...   \n",
       "\n",
       "                                               attrs    price  is_proved  \\\n",
       "0  {\"Год выпуска\":\"1991\", \"Тип кузова\":\"Купе\", \"П...   150000        NaN   \n",
       "1  {\"Вид одежды\":\"Женская одежда\", \"Предмет одежд...     1500        NaN   \n",
       "2  {\"Вид одежды\":\"Для мальчиков\", \"Предмет одежды...     3000        NaN   \n",
       "3  {\"Тип объявления\":\"Продам\", \"Количество комнат...  2642020        NaN   \n",
       "4  {\"Вид услуги\":\"Деловые услуги\", \"Тип услуги\":\"...     1500        0.0   \n",
       "\n",
       "   is_blocked  phones_cnt  emails_cnt  urls_cnt  close_hours  \n",
       "0           0           0           0         0         0.03  \n",
       "1           0           0           0         0         0.41  \n",
       "2           0           0           0         0         5.49  \n",
       "3           0           1           0         0        22.47  \n",
       "4           1           0           0         0         1.43  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape, df.is_blocked.mean())\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](https://kaggle2.blob.core.windows.net/competitions/kaggle/3929/media/Ad.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocked ratio 0.228222107326\n",
      "Count: 1204949\n"
     ]
    }
   ],
   "source": [
    "print(\"Blocked ratio\",df.is_blocked.mean())\n",
    "print(\"Count:\",len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance-out the classes\n",
    "* Vast majority of data samples are non-prohibited\n",
    " * 250k banned out of 4kk\n",
    " * Let's just downsample random 250k legal samples to make further steps less computationally demanding\n",
    " * If you aim for high Kaggle score, consider a smarter approach to that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocked ratio: 0.5\n",
      "Count: 500000\n"
     ]
    }
   ],
   "source": [
    "#downsample\n",
    "\n",
    "\n",
    "# < downsample data so that both classes have approximately equal ratios>\n",
    "legal_indices = df[df['is_blocked'] == 0].index\n",
    "illegal_indices = df[df['is_blocked'] == 1].index\n",
    "chosen_legal_indices = np.random.choice(legal_indices, 250000)\n",
    "chosen_illegal_indices = np.random.choice(illegal_indices, 250000)\n",
    "df = pd.concat([df.iloc[chosen_illegal_indices], df.iloc[chosen_legal_indices]])\n",
    "\n",
    "\n",
    "print(\"Blocked ratio:\",df.is_blocked.mean())\n",
    "print(\"Count:\",len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed\n"
     ]
    }
   ],
   "source": [
    "assert df.is_blocked.mean() < 0.51\n",
    "assert df.is_blocked.mean() > 0.49\n",
    "assert len(df) <= 560000\n",
    "\n",
    "print(\"All tests passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#In case your RAM-o-meter is in the red\n",
    "if very_low_RAM:\n",
    "    data = data[::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Tokenizing\n",
    "\n",
    "First, we create a dictionary of all existing words.\n",
    "Assign each word a number - it's Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter,defaultdict\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "#Dictionary of tokens\n",
    "token_counts = Counter()\n",
    "\n",
    "#All texts\n",
    "all_texts = np.hstack([df.description.values,df.title.values])\n",
    "\n",
    "\n",
    "#Compute token frequencies\n",
    "for s in all_texts:\n",
    "    if type(s) is not str:\n",
    "        continue\n",
    "    s = s.lower()\n",
    "    tokens = tokenizer.tokenize(s)\n",
    "    for token in tokens:\n",
    "        token_counts[token] +=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove rare tokens\n",
    "\n",
    "We are unlikely to make use of words that are only seen a few times throughout the corpora.\n",
    "\n",
    "Again, if you want to beat Kaggle competition metrics, consider doing something better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAFkCAYAAAAKf8APAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAIABJREFUeJzt3X20XmV95//3hwdDoRKcSZPgVH6WWjF0FMnxgYyK2gww\nCD50bAePzfK5oxXUhp9Ibe2QgTo/xQUBRFpGdAmip4sBi1iQKOiAFRQhQulwTO2IAtJET4XEIuEp\n398fe9/lzt2Tk5Nwn5NN8n6tda+c+7q+997X3isr+ZxrX3vfqSokSZK6ZLcdPQBJkqRBBhRJktQ5\nBhRJktQ5BhRJktQ5BhRJktQ5BhRJktQ5BhRJktQ5BhRJktQ5BhRJktQ5BhRJktQ52xxQkrwsyRVJ\nfpxkU5LXDPTvk+TcJHcn+UWS/5PknQM1c5J8IslEkp8nuTTJ/IGaZyS5MskDSdYmOT3JbgM1r0hy\nS5KNSf4+yZsnGe/xSe5M8mCSbyV54bYesyRJml3bM4OyD3ArcDww2Rf5rASOBN4IPAc4Czg3ybF9\nNWcBxwCvBw4Hng5c1utsg8hVwB7AYcCbgbcAp/bVPBP4a+Ba4BDgbOCCJEf01RwHnAGcAhwK3Aas\nSjJvO45bkiTNkjyRLwtMsgl4XVVd0dd2O/CXVfXhvrabgauq6r8l2Rf4KfCGqvqrtv8gYBw4rKpu\nSnI0cAWwf1VNtDXvBD4C/EpVPZrko8DRVfW8vv2MAXOr6lXt+28B366q97XvA9wNnFNVp2/3gUuS\npBk1E2tQbgBek+TpAEleCfwGsKrtH6GZGbm294GqWgPcBSxpmw4Dbu+Fk9YqYC7wm3011wzse1Vv\nG0n2bPfVv59qP7MESZLUWXvMwDbfA/xP4J4kjwKPAb9fVd9s+xcCD1fVhoHPrWv7ejXrJunv9d02\nRc2+SeYA/wbYfQs1B0028CT/FjgK+CGwccuHKEmSBuwFPBNYVVX/9EQ3NhMB5b3Ai4FjaWZFDgfO\nS3JvVX1tis+Fyde0DJqqJtOs2VL/UcDnpjEGSZI0ud8DPv9ENzLUgJJkL+DDwGur6uq2+e+SHAq8\nH/gasBZ4SpJ9B2ZR5vP4bMdaYPBumwV9fb0/FwzUzAc2VNXDSSZoZm8mqxmcVen5IcDFF1/MokWL\ntnicGq7ly5ezcuXKHT2MXYrnfPZ5zmef53x2jY+Ps2zZMmj/L32ihj2Dsmf7GpyheIzH17vcAjwK\nLAV6i2SfDRxAs34F4Ebgj5PM61uHciSwnmYxba/m6IH9HNm2U1WPJLml3c8V7X7Svj9nC+PfCLBo\n0SIWL148vSPWEzZ37lzP9yzznM8+z/ns85zvMENZIrHNASXJPsCzePxyyoFJDgF+VlV3J7kO+FiS\njcCPgFcAbwL+EKCqNiT5FHBmkvuAn9MEhm9W1XfabX4FuAP4bJKTgf2B04Bzq+qRtuYvgBPau3k+\nTRM8fgd4Vd9wzwQubIPKTcByYG/gM9t63JIkafZszwzKC4Cv08ySFM1zRgAuBN4GHAf8f8DFNAtV\nfwR8sKr+Z982ltPMqlwKzAGupnmuCgBVtal9bsqf08yqPEATKk7pq/lhkmNoQsh7gXuAt1fVNX01\nl7TPPDmV5lLPrcBRVfXT7ThuSZI0S7Y5oFTVdUxxe3JV/QR4+1a28RDN3T7vmaLmbpqFtlsby8hW\nas4DzpuqRpIkdYvfxaMdbnR0dEcPYZfjOZ99nvPZ5zl/cntCT5Ld2SRZDNxyyy23uLBKkqRtsHr1\nakZGRgBGqmr1E92eMyiSJKlzDCiSJKlzDCiSJKlzDCiSJKlzDCiSJKlzDCiSJKlzDCiSJKlzDCiS\nJKlzDCiSJKlzDCiSJKlzDCiSJKlzDCiSJKlzDCiSJKlzDCiSJKlzDCiSJKlzDCiSJKlz9tjRA3iy\nuuuuu5iYmJiyZt68eRxwwAGzNCJJknYeBpTtcNddd3HQQYvYuPEXU9bttdferFkzbkiRJGkbGVC2\nw8TERBtOLgYWbaFqnI0blzExMWFAkSRpGxlQnpBFwOIdPQhJknY6LpKVJEmds80BJcnLklyR5MdJ\nNiV5zSQ1i5J8Mcn9Sf45ybeT/Gpf/5wkn0gykeTnSS5NMn9gG89IcmWSB5KsTXJ6kt0Gal6R5JYk\nG5P8fZI3TzKW45PcmeTBJN9K8sJtPWZJkjS7tmcGZR/gVuB4oAY7k/w68A3gDuBw4LnAacDGvrKz\ngGOA17c1Twcu69vGbsBVNJegDgPeDLwFOLWv5pnAXwPXAocAZwMXJDmir+Y44AzgFOBQ4DZgVZJ5\n23HckiRplmzzGpSquhq4GiBJJin5M+DKqvpgX9udvR+S7Au8DXhDVV3Xtr0VGE/yoqq6CTgKeA7w\nyqqaAG5P8qfAR5KsqKpHgT8AflBVH2g3vSbJS4HlwFfbtuXA+VV1Ubufd9EEo7cBp2/rsUuSpNkx\n1DUobWA5Bvh+kquTrGsvq7y2r2yEJhhd22uoqjXAXcCStukw4PY2nPSsAuYCv9lXc83AEFb1tpFk\nz3Zf/fup9jNLkCRJnTXsRbLzgV8GTqa5RHME8FfAF5K8rK1ZCDxcVRsGPruu7evVrJukn2nU7Jtk\nDjAP2H0LNQuRJEmdNezbjHuB5/KqOqf9+W+T/AfgXTRrU7YkTLKmZRJT1WSaNdPZjyRJ2kGGHVAm\ngEeB8YH2ceAl7c9rgack2XdgFmU+j892rAUG77ZZ0NfX+3PBQM18YENVPZxkAnhsCzWDsyqbWb58\nOXPnzt2sbXR0lNHR0ak+JknSLmFsbIyxsbHN2tavXz/UfQw1oFTVI0m+Axw00PVs4Eftz7fQhJil\nNJd/SPJs4ADghrbmRuCPk8zrW4dyJLCex8PPjcDRA/s5sm3vjeWWdj9XtPtJ+/4cprBy5UoWL/YB\nbJIkTWayX9pXr17NyMjI0PaxzQElyT7As3j8csqBSQ4BflZVdwMfA/4yyTeAr9OEiGOBlwNU1YYk\nnwLOTHIf8HOawPDNqvpOu82v0Nym/NkkJwP709yqfG5VPdLW/AVwQpKPAp+mCR6/A7yqb7hnAhe2\nQeUmmrt69gY+s63HLUmSZs/2zKC8gCZ4VPs6o22/EHhbVV3e3s77xzTPJlkD/OequrFvG8tpLr9c\nCsyhuW35+F5nVW1Kcizw5zSzKg/QhIpT+mp+mOQYmhDyXuAe4O1VdU1fzSXtM09OpbnUcytwVFX9\ndDuOW5IkzZLteQ7KdWzl7p+q+gxTzFJU1UPAe9rXlmruppl52dpYppxPqqrzgPOmqpEkSd3id/FI\nkqTOMaBIkqTOMaBIkqTOMaBIkqTOMaBIkqTOMaBIkqTOMaBIkqTOMaBIkqTOMaBIkqTOMaBIkqTO\nMaBIkqTOMaBIkqTOMaBIkqTOMaBIkqTOMaBIkqTOMaBIkqTOMaBIkqTOMaBIkqTOMaBIkqTOMaBI\nkqTOMaBIkqTOMaBIkqTOMaBIkqTOMaBIkqTO2eaAkuRlSa5I8uMkm5K8Zora89ua9w60Py3J55Ks\nT3JfkguS7DNQ87wk1yd5MMmPkpw0yfZ/N8l4W3NbkqMnqTk1yb1JfpHkq0meta3HLEmSZtf2zKDs\nA9wKHA/UloqSvA54EfDjSbo/DywClgLHAIcD5/d99qnAKuBOYDFwErAiyTv6apa02/kk8HzgcuDy\nJAf31ZwMnAC8sx3LA8CqJE/Z1oOWJEmzZ49t/UBVXQ1cDZAkk9Uk+XfAOcBRwFUDfc9p20eq6rtt\n23uAK5O8v6rWAsuAPYG3V9WjwHiSQ4ETgQvaTb0P+HJVndm+PyXJkTSB5N19NadV1Zfa/bwJWAe8\nDrhkW49dkiTNjqGvQWlDy0XA6VU1PknJEuC+XjhpXUMzG/Pi9v1hwPVtOOlZBRyUZG7fdq4Z2Paq\ntp0kBwILgWt7nVW1Afh2r0aSJHXTTCyS/SPg4ao6dwv9C4Gf9DdU1WPAz9q+Xs26gc+t6+ubqqbX\nv4Am9ExVI0mSOmibL/FMJckI8F7g0O35OFOsaWn7p1MzVf+0apYvX87cuXM3axsdHWV0dHQrm5Yk\naec3NjbG2NjYZm3r168f6j6GGlCAlwK/Atzdtzxld+DMJH9YVQcCa4H5/R9KsjvwtLaP9s8FA9ue\nz+YzIluq6e9PW7NuoOa7TGHlypUsXrx4qhJJknZZk/3Svnr1akZGRoa2j2Ff4rkIeB5wSN/rXuB0\nmoWxADcC+7WLXnuW0oSJm/pqDm+DS8+RwJqqWt9Xs3Rg/0e07VTVnTQh5V9qkuxLs87lhu0/REmS\nNNO2eQalfV7Js2gCBcCBSQ4BflZVdwP3DdQ/Aqytqu8DVNX3kqwCPpnkD4CnAB8Hxto7eKC5ffi/\nAZ9O8lHguTSXjt7Xt+mzgeuSnAhcCYwCI8Dv99WcBXwoyT8APwROA+4Bvritxy1JkmbP9lzieQHw\ndZrLLQWc0bZfCLxtkvrJ1nu8ETiX5i6cTcCl9IWPqtqQ5Ki25mZgAlhRVZ/qq7kxySjw4fb1feC1\nVXVHX83pSfamecbKfsA3gKOr6uHtOG5JkjRLtuc5KNexDZeG2nUng2330zzrZKrP3Q68fCs1lwGX\nbaVmBbBia+OUJEnd4XfxSJKkzjGgSJKkzjGgSJKkzjGgSJKkzjGgSJKkzjGgSJKkzjGgSJKkzjGg\nSJKkzjGgSJKkzjGgSJKkzjGgSJKkzjGgSJKkzjGgSJKkzjGgSJKkzjGgSJKkzjGgSJKkzjGgSJKk\nzjGgSJKkzjGgSJKkzjGgSJKkzjGgSJKkzjGgSJKkzjGgSJKkztnmgJLkZUmuSPLjJJuSvKavb48k\nH03yt0n+ua25MMn+A9t4WpLPJVmf5L4kFyTZZ6DmeUmuT/Jgkh8lOWmSsfxukvG25rYkR09Sc2qS\ne5P8IslXkzxrW49ZkiTNru2ZQdkHuBU4HqiBvr2B5wP/HTgU+G3gIOCLA3WfBxYBS4FjgMOB83ud\nSZ4KrALuBBYDJwErkryjr2ZJu51Ptvu8HLg8ycF9NScDJwDvBF4EPACsSvKU7ThuSZI0S/bY1g9U\n1dXA1QBJMtC3ATiqvy3JCcC3k/xqVd2TZFFbM1JV321r3gNcmeT9VbUWWAbsCby9qh4FxpMcCpwI\nXNBu+n3Al6vqzPb9KUmOpAkk7+6rOa2qvtTu503AOuB1wCXbeuySJGl2zMYalP1oZlrub98fBtzX\nCyeta9qaF/fVXN+Gk55VwEFJ5rbvl7SfY6BmCUCSA4GFwLW9zjZAfbtXI0mSumlGA0qSOcBHgM9X\n1T+3zQuBn/TXVdVjwM/avl7NuoHNrevrm6qm17+AJvRMVSNJkjpoxgJKkj2A/0UTEt69lXKA8K/X\ntAz2T6dmqv7p1kiSpB1om9egTEdfOHkG8Ft9sycAa4H5A/W7A09r+3o1CwY2O5/NZ0S2VNPfn7Zm\n3UDNd5nC8uXLmTt37mZto6OjjI6OTvUxSZJ2CWNjY4yNjW3Wtn79+qHuY+gBpS+cHAi8sqruGyi5\nEdgvyaF961CW0oSJm/pq/izJ7u3lH4AjgTVVtb6vZilwTt+2j2jbqao7k6xta/62Hdu+NOtcPjHV\nMaxcuZLFixdvw1FLkrTrmOyX9tWrVzMyMjK0fWzPc1D2SXJIkue3TQe275/RzoRcRnNr8DJgzyQL\n2teeAFX1PZrFrJ9M8sIkLwE+Doy1d/BAc/vww8Cnkxyc5DjgvcAZfUM5Gzg6yYlJDkqyAhgBzu2r\nOQv4UJJXJ3kucBFwD//6tmdJktQh2zOD8gLg6zSXW4rHQ8OFNM8/eXXbfmvb3lvz8Urg+rbtjTRB\n4hpgE3ApzS3BQHO3TZKj2pqbgQlgRVV9qq/mxiSjwIfb1/eB11bVHX01pyfZm+YZK/sB3wCOrqqH\nt+O4JUnSLNme56Bcx9QzL1udlamq+2lmWKaquR14+VZqLqOZsZmqZgWwYmtjkiRJ3eF38UiSpM4x\noEiSpM4xoEiSpM4xoEiSpM4xoEiSpM4xoEiSpM4xoEiSpM4xoEiSpM4xoEiSpM4xoEiSpM4xoEiS\npM4xoEiSpM4xoEiSpM4xoEiSpM4xoEiSpM4xoEiSpM4xoEiSpM4xoEiSpM4xoEiSpM4xoEiSpM4x\noEiSpM4xoEiSpM4xoEiSpM4xoEiSpM7Z5oCS5GVJrkjy4ySbkrxmkppTk9yb5BdJvprkWQP9T0vy\nuSTrk9yX5IIk+wzUPC/J9UkeTPKjJCdNsp/fTTLe1tyW5OhtHYskSeqe7ZlB2Qe4FTgeqMHOJCcD\nJwDvBF4EPACsSvKUvrLPA4uApcAxwOHA+X3beCqwCrgTWAycBKxI8o6+miXtdj4JPB+4HLg8ycHb\nOBZJktQxe2zrB6rqauBqgCSZpOR9wGlV9aW25k3AOuB1wCVJFgFHASNV9d225j3AlUneX1VrgWXA\nnsDbq+pRYDzJocCJwAV9+/lyVZ3Zvj8lyZE0geTd0xnLth67JEmaHUNdg5Lk14CFwLW9tqraAHwb\nWNI2HQbc1wsnrWtoZmNe3FdzfRtOelYBByWZ275f0n6OgZol7VgOnMZYJElSBw17kexCmqCxbqB9\nXdvXq/lJf2dVPQb8bKBmsm0wjZpe/4JpjEWSJHXQNl/i2U5hkvUq21iTadY80f2wfPly5s6du1nb\n6Ogoo6OjW9m0JEk7v7GxMcbGxjZrW79+/VD3MeyAspYmACxg85mL+cB3+2rm938oye7A09q+Xs2C\ngW3PZ/MZkS3V9PdvbSyTWrlyJYsXL56qRJKkXdZkv7SvXr2akZGRoe1jqJd4qupOmmCwtNeWZF+a\ntSU3tE03Avu1i157ltKEiZv6ag5vg0vPkcCaqlrfV7OUzR3Rtk93LJIkqYO25zko+yQ5JMnz26YD\n2/fPaN+fBXwoyauTPBe4CLgH+CJAVX2PZjHrJ5O8MMlLgI8DY+0dPNDcPvww8OkkByc5DngvcEbf\nUM4Gjk5yYpKDkqwARoBz+2qmHIskSeqm7bnE8wLg6zSXW4rHQ8OFwNuq6vQke9M812Q/4BvA0VX1\ncN823kgTJK4BNgGX0twSDDR32yQ5qq25GZgAVlTVp/pqbkwyCny4fX0feG1V3dFXM52xSJKkjtme\n56Bcx1ZmXqpqBbBiiv77aZ51MtU2bgdevpWay4DLnshYJElS9/hdPJIkqXMMKJIkqXMMKJIkqXMM\nKJIkqXMMKJIkqXMMKJIkqXMMKJIkqXMMKJIkqXMMKJIkqXMMKJIkqXMMKJIkqXMMKJIkqXMMKJIk\nqXMMKJIkqXMMKJIkqXMMKJIkqXMMKJIkqXMMKJIkqXMMKJIkqXMMKJIkqXMMKJIkqXMMKJIkqXMM\nKJIkqXOGHlCS7JbktCQ/SPKLJP+Q5EOT1J2a5N625qtJnjXQ/7Qkn0uyPsl9SS5Iss9AzfOSXJ/k\nwSQ/SnLSJPv53STjbc1tSY4e9jFLkqThmokZlD8C3gm8G3gO8AHgA0lO6BUkORk4oa17EfAAsCrJ\nU/q283lgEbAUOAY4HDi/bxtPBVYBdwKLgZOAFUne0VezpN3OJ4HnA5cDlyc5eLiHLEmShmkmAsoS\n4ItVdXVV3VVVXwC+QhNEet4HnFZVX6qqvwPeBDwdeB1AkkXAUcDbq+rmqroBeA/whiQL220sA/Zs\na8ar6hLgHODEgf18uarOrKo1VXUKsJomHEmSpI6aiYByA7A0yW8AJDkEeAlwVfv+14CFwLW9D1TV\nBuDbNOEG4DDgvqr6bt92rwEKeHFfzfVV9WhfzSrgoCRz2/dL2s8xULMESZLUWXvMwDY/AuwLfC/J\nYzQh6E+q6i/b/oU0QWPdwOfWtX29mp/0d1bVY0l+NlDzg0m20etb3/451X4kSVIHzURAOQ54I/AG\n4A6atR9nJ7m3qj47xedCE1ymsrWaTLNma/uRJEk70EwElNOB/1FV/6t9/3+SPBP4IPBZYC1NSFjA\n5rMb84HeJZ217ft/kWR34GltX69mwcC+57P57MyWagZnVTazfPly5s6du1nb6Ogoo6OjU31MkqRd\nwtjYGGNjY5u1rV+/fqj7mImAsjf/eoZiE+16l6q6M8lamrtz/hYgyb40a0s+0dbfCOyX5NC+dShL\naYLNTX01f5Zk96p6rG07ElhTVev7apbSLJ7tOaJt36KVK1eyePHiaR6uJEm7lsl+aV+9ejUjIyND\n28dMLJL9EvAnSV6V5P9J8tvAcuALfTVnAR9K8uokzwUuAu4BvghQVd+jWcz6ySQvTPIS4OPAWFX1\nZlA+DzwMfDrJwUmOA94LnNG3n7OBo5OcmOSgJCuAEeDcGThuSZI0JDMxg3ICcBrNbMh84F7gz9s2\nAKrq9CR70zzXZD/gG8DRVfVw33beSBMkrqGZgbmU5rbh3jY2JDmqrbkZmABWVNWn+mpuTDIKfLh9\nfR94bVXdMeyD3pLx8fEp++fNm8cBBxwwS6ORJOnJYegBpaoeoHkWyYlbqVsBrJii/36aZ51MtY3b\ngZdvpeYy4LKpambGPwK7sWzZlIfAXnvtzZo144YUSZL6zMQMigC4n2bi52KaB+JOZpyNG5cxMTFh\nQJEkqY8BZcYtonkSvyRJmi6/zViSJHWOAUWSJHWOAUWSJHWOAUWSJHWOAUWSJHWOAUWSJHWOAUWS\nJHWOAUWSJHWOAUWSJHWOAUWSJHWOAUWSJHWOAUWSJHWOAUWSJHWOAUWSJHWOAUWSJHWOAUWSJHWO\nAUWSJHWOAUWSJHWOAUWSJHWOAUWSJHWOAUWSJHWOAUWSJHXOjASUJE9P8tkkE0l+keS2JIsHak5N\ncm/b/9Ukzxrof1qSzyVZn+S+JBck2Weg5nlJrk/yYJIfJTlpkrH8bpLxtua2JEfPxDFLkqThGXpA\nSbIf8E3gIeAoYBHw/wL39dWcDJwAvBN4EfAAsCrJU/o29fn2s0uBY4DDgfP7tvFUYBVwJ7AYOAlY\nkeQdfTVL2u18Eng+cDlweZKDh3rQkiRpqPaYgW3+EXBXVb2jr+1HAzXvA06rqi8BJHkTsA54HXBJ\nkkU04Wakqr7b1rwHuDLJ+6tqLbAM2BN4e1U9CownORQ4Ebigbz9frqoz2/enJDmSJhy9e6hHLUmS\nhmYmLvG8Grg5ySVJ1iVZPTCr8WvAQuDaXltVbQC+DSxpmw4D7uuFk9Y1QAEv7qu5vg0nPauAg5LM\nbd8vaT/HQM0SJElSZ81EQDkQ+ANgDXAk8BfAOUmWtf0LaYLGuoHPrWv7ejU/6e+sqseAnw3UTLYN\nplGzEEmS1FkzcYlnN+CmqvrT9v1tSX6TJrRcPMXnQhNcprK1mkyzZsr9LF++nLlz527WNjo6yujo\n6FaGJ0nSzm9sbIyxsbHN2tavXz/UfcxEQPlHYHygbRz4z+3Pa2lCwgI2n92YD3y3r2Z+/waS7A48\nre3r1SwY2M98Np+d2VLN4KzKZlauXMnixYunKpEkaZc12S/tq1evZmRkZGj7mIlLPN8EDhpoO4h2\noWxV3UkTHJb2OpPsS7O25Ia26UZgv3bRa89SmmBzU1/N4W1w6TkSWFNV6/tqlrK5I9p2SZLUUTMR\nUFYChyX5YJJfT/JG4B3AuX01ZwEfSvLqJM8FLgLuAb4IUFXfo1nM+skkL0zyEuDjwFh7Bw80tw8/\nDHw6ycFJjgPeC5zRt5+zgaOTnJjkoCQrgJGBsUiSpI4ZekCpqpuB3wZGgduBPwHeV1V/2VdzOk3g\nOJ/m7p1fAo6uqof7NvVG4Hs0d+H8NXA9zXNTetvYQHMr8jOBm4GPASuq6lN9NTe24/ivwK00l5le\nW1V3DPWgJUnSUM3EGhSq6irgqq3UrABWTNF/P82zTqbaxu3Ay7dScxlw2VQ1kiSpW/wuHkmS1DkG\nFEmS1DkGFEmS1DkGFEmS1DkGFEmS1DkGFEmS1DkGFEmS1DkGFEmS1DkGFEmS1DkGFEmS1DkGFEmS\n1DkGFEmS1DkGFEmS1DkGFEmS1DkGFEmS1DkGFEmS1DkGFEmS1DkGFEmS1DkGFEmS1DkGFEmS1DkG\nFEmS1DkGFEmS1DkGFEmS1DkzHlCSfDDJpiRn9rXNSfKJJBNJfp7k0iTzBz73jCRXJnkgydokpyfZ\nbaDmFUluSbIxyd8nefMk+z8+yZ1JHkzyrSQvnLmjlSRJwzCjAaUNA78P3DbQdRZwDPB64HDg6cBl\nfZ/bDbgK2AM4DHgz8Bbg1L6aZwJ/DVwLHAKcDVyQ5Ii+muOAM4BTgEPbcaxKMm9oBylJkoZuxgJK\nkl8GLgbeAdzf174v8DZgeVVdV1XfBd4KvCTJi9qyo4DnAL9XVbdX1SrgT4Hjk+zR1vwB8IOq+kBV\nramqTwCXAsv7hrEcOL+qLqqq7wHvAn7R7l+SJHXUTM6gfAL4UlV9baD9BTQzI9f2GqpqDXAXsKRt\nOgy4vaom+j63CpgL/GZfzTUD217V20aSPYGRgf1U+5klSJKkztpj6yXbLskbgOfThJFBC4CHq2rD\nQPs6YGH788L2/WB/r++2KWr2TTIH+DfA7luoOWh6RyJJknaEoQeUJL9Ks8bkiKp6ZFs+CtQ06qaq\nyTRrprMfSZK0g8zEDMoI8CvALUl6gWF34PAkJwD/CZiTZN+BWZT5PD7bsRYYvNtmQV9f788FAzXz\ngQ1V9XCSCeCxLdQMzqpsZvny5cydO3ezttHRUUZHR6f6mCRJu4SxsTHGxsY2a1u/fv1Q9zETAeUa\n4LkDbZ8BxoGPAD8GHgGWAn8FkOTZwAHADW39jcAfJ5nXtw7lSGB9u51ezdED+zmybaeqHklyS7uf\nK9r9pH1/zlQHsHLlShYvXjy9o5UkaRcz2S/tq1evZmRkZGj7GHpAqaoHgDv625I8APxTVY237z8F\nnJnkPuDnNIHhm1X1nfYjX2m38dkkJwP7A6cB5/ZdNvoL4IQkHwU+TRM8fgd4Vd+uzwQubIPKTTR3\n9exNE5gkSVJHzcgi2UkMrvlYTnP55VJgDnA1cPy/FFdtSnIs8Oc0syoP0ISKU/pqfpjkGJoQ8l7g\nHuDtVXVNX80l7TNPTqW51HMrcFRV/XTYByhJkoZnVgJKVf3WwPuHgPe0ry195m7g2K1s9zqaNS9T\n1ZwHnDd6BcPPAAAMwElEQVTtwUqSpB3O7+KRJEmdM1uXeDSF8fHxrdbMmzePAw44YBZGI0nSjmdA\n2aH+EdiNZcuWbbVyr732Zs2acUOKJGmXYEDZoe4HNtF8ZdGiKerG2bhxGRMTEwYUSdIuwYDSCYsA\nn7siSVKPi2QlSVLnGFAkSVLnGFAkSVLnGFAkSVLnGFAkSVLnGFAkSVLnGFAkSVLnGFAkSVLnGFAk\nSVLnGFAkSVLnGFAkSVLnGFAkSVLnGFAkSVLnGFAkSVLnGFAkSVLnGFAkSVLnGFAkSVLn7LGjB6Dp\nGx8fn7J/3rx5HHDAAbM0GkmSZo4B5UnhH4HdWLZs2ZRVe+21N2vWjBtSJElPekO/xJPkg0luSrIh\nybokf5Xk2QM1c5J8IslEkp8nuTTJ/IGaZyS5MskDSdYmOT3JbgM1r0hyS5KNSf4+yZsnGc/xSe5M\n8mCSbyV54bCPeebdD2wCLgZu2cLrYjZu/AUTExM7bJSSJA3LTKxBeRnwceDFwH8E9gS+kuSX+mrO\nAo4BXg8cDjwduKzX2QaRq2hmeA4D3gy8BTi1r+aZwF8D1wKHAGcDFyQ5oq/mOOAM4BTgUOA2YFWS\necM73Nm0CFi8hdeiHTguSZKGa+iXeKrqVf3vk7wF+AkwAvxNkn2BtwFvqKrr2pq3AuNJXlRVNwFH\nAc8BXllVE8DtSf4U+EiSFVX1KPAHwA+q6gPtrtYkeSmwHPhq27YcOL+qLmr38y6aYPQ24PRhH7sk\nSRqO2biLZz+ggJ+170dogtG1vYKqWgPcBSxpmw4Dbm/DSc8qYC7wm3011wzsa1VvG0n2bPfVv59q\nP7MESZLUWTMaUJKE5nLO31TVHW3zQuDhqtowUL6u7evVrJukn2nU7JtkDjAP2H0LNQuRJEmdNdN3\n8ZwHHAy8dBq1oZlp2ZqpajLNmin3s3z5cubOnbtZ2+joKKOjo9MYniRJO7exsTHGxsY2a1u/fv1Q\n9zFjASXJucCrgJdV1b19XWuBpyTZd2AWZT6Pz3asBQbvtlnQ19f7c8FAzXxgQ1U9nGQCeGwLNYOz\nKptZuXIlixcvnqpEkqRd1mS/tK9evZqRkZGh7WNGLvG04eS1NItc7xrovgV4FFjaV/9s4ADghrbp\nRuC5A3fbHAmsB8b7apayuSPbdqrqkXZf/ftJ+/4GJElSZw19BiXJecAo8BrggSS9GYz1VbWxqjYk\n+RRwZpL7gJ8D5wDfrKrvtLVfAe4APpvkZGB/4DTg3DZ4APwFcEKSjwKfpgkev0Mza9NzJnBhkluA\nm2ju6tkb+Mywj1uSJA3PTFzieRfNGo//PdD+VuCi9uflNJdfLgXmAFcDx/cKq2pTkmOBP6eZ7XiA\nJlSc0lfzwyTH0ISQ9wL3AG+vqmv6ai5pZ2FOpbnUcytwVFX9dEjHKkmSZsBMPAdlq5eNquoh4D3t\na0s1dwPHbmU719HcSjxVzXk0i3UlSdKThN9mLEmSOseAIkmSOseAIkmSOmemH9SmWTY+Pj5l/7x5\n8zjggANmaTSSJG0fA8pO4x+B3Vi2bNmUVXvttTdr1owbUiRJnWZA2WncD2wCLgYWbaFmnI0blzEx\nMWFAkSR1mgFlp7MI8DH9kqQnNxfJSpKkznEGZRfkQlpJUtcZUHYpLqSVJD05GFB2KS6klSQ9ORhQ\ndkkupJUkdZuLZCVJUuc4g6JJbW0hLbiYVpI0cwwoGjC9hbTgYlpJ0swxoGjAdBbSQm8x7Te+8Q0W\nLdpynbMskqTtYUDRFmxtIa23LEuSZo4BRdvJW5YlSTPHgKInaOu3LPvkWknStjKgaAZN7zLQnDl7\ncdlll7L//vtPWWeQkaRdhwFFM2g6l4G+wUMPncixxx671a1NJ8gYYiRp52BA0SyY6jLQONO7a2h6\nQcYQI0k7BwOKOmJra1mmE2SGF2IAHnroIebMmTNlzZM17IyNjTE6Orqjh7FL8ZzPPs/5k9suEVCS\nHA+8H1gI3Aa8p6q+s2NHpe3zRGdjpn9JCXYHHpuyYjphp4tBx3+4Z5/nfPZ5zp/cdvqAkuQ44Azg\nvwI3AcuBVUmeXVUTO3RwmiHDuKR0FfCnW6mbbtgZTtCB6YWd6dQ8+OCDU/ZL0o620wcUmkByflVd\nBJDkXcAxwNuA03fkwLQjTeeS0tbqphN2hhl0YDphZzo1u+22G1deeeWsBaLp1AxzW0/WS2+SHrdT\nB5QkewIjwP/otVVVJbkGWLLDBqadzNZCzHRqhjWrM71AtGnTH85qIJpezfC2NaxLb9Otm07Nfffd\nx+rVq2dtf7N9fLM9pumE0AcffHBa59xAOz133XUXExNbvvAwnS+Z3RY7dUAB5tH8a7ZuoH0dcNAk\n9XsBfOELX+Dmm2/e4kY3bNjQ/nQVj/8HNOibQ6oZ5raerGPa2ffXq7lzivEA3DuNuunUrGn/fDsw\n1QzK7cAXt1I3rJphbuv7PPTQJdMIYLvRBMOtmU7d9LY1MjIyi/ub7eOb3THtueccPvaxjzJv3rxJ\n+ycmJrj22q9N65xvbVvQzDpu2jT1mKZTM8xtzeaYJiYmOOmkP+KRRzZudVu0/5c+UamqYWynk5Ls\nD/wYWFJV3+5rPx14aVX9h4H6NwKfm91RSpK0U/m9qvr8E93Izj6DMkEzF7xgoH0+/3pWBWAV8HvA\nD4FpxURJkgQ0MyfPpPm/9AnbqWdQAJJ8C/h2Vb2vfR/gLuCcqvrYDh2cJEma1M4+gwJwJnBhklt4\n/DbjvYHP7MhBSZKkLdvpA0pVXZJkHnAqzaWeW4GjquqnO3ZkkiRpS3b6SzySJOnJZ7cdPQBJkqRB\nBhRJktQ5BpQ+SY5PcmeSB5N8K8kLd/SYdhZJXpbkiiQ/TrIpyWsmqTk1yb1JfpHkq0metSPGurNI\n8sEkNyXZkGRdkr9K8uyBmjlJPpFkIsnPk1yaZP6OGvOTXZJ3Jbktyfr2dUOS/9TX7/meQe3f+U1J\nzuxr85wPWZJT2vPc/7qjr38o59yA0ur7UsFTgENpvvV4VbvAVk/cPjQLlI8H/tXCpyQnAycA7wRe\nBDxAc/6fMpuD3Mm8DPg48GLgPwJ7Al9J8kt9NWfRfDfV64HDgacDl83yOHcmdwMn03zFxgjwNeCL\nSXrfO+D5niHtL5S/T/Nvdz/P+cz4O5obTxa2r5f29Q3nnFeVr2ah8LeAs/veB7gH+MCOHtvO9qJ5\nhvVrBtruBZb3vd8XeBD4Lzt6vDvLi+arHzbRPEW5d44fAn67r+agtuZFO3q8O8sL+CfgrZ7vGT3H\nv0zzHQ6/BXwdOLNt95zPzPk+BVi9hb6hnXNnUNjsSwWv7bVVc1b9UsFZkOTXaBJ4//nfAHwbz/8w\n7Ucze/Wz9v0IzaMG+s/7GpoHGXren6AkuyV5A81zl27E8z2TPgF8qaq+NtD+AjznM+U32kv2/zfJ\nxUme0bYP7e/5Tv8clGna1i8V1HAtpPmPc7Lzv3D2h7PzaZ+gfBbwN1XVu1a8EHi4DYP9PO9PQJJ/\nTxNI9gJ+TvOb5PeSHIrne+jaEPh8mjAyaAGe85nwLeAtNLNW+wMrgOvbv/tD+3fFgDK1MMl6Cc0a\nz//wnAcczObXibfE8/7EfA84hGbG6vXARUkOn6Le872dkvwqTfA+oqoe2ZaP4jnfblXV/107f5fk\nJuBHwH9hy99jt83n3Es8jW39UkEN11qav7ye/xmQ5FzgVcArqurevq61wFOS7DvwEc/7E1BVj1bV\nD6pqdVX9Cc2izffh+Z4JI8CvALckeSTJI8DLgfcleZjmvM7xnM+sqloP/D3wLIb499yAArTJ+xZg\naa+tnRJfCtywo8a1q6iqO2n+Uvef/31p7j7x/D8BbTh5LfDKqrproPsW4FE2P+/PBg6guUSh4dgN\nmIPneyZcAzyX5hLPIe3rZuDivp8fwXM+o5L8MvDrNDc7DO3vuZd4HueXCs6gJPvQpOu0TQcmOQT4\nWVXdTTNN+6Ek/wD8EDiN5i6qL+6A4e4UkpwHjAKvAR5I0puhWl9VG6tqQ5JPAWcmuY9mvcQ5wDer\n6qYdM+ontyQfBr5Mc7vxU4Hfo/mN/kjP9/BV1QPAHf1tSR4A/qmqxtv3nvMhS/Ix4Es0l3X+HfDf\naULJXw7z77kBpVV+qeBMewHN7X/Vvs5o2y8E3lZVpyfZGzif5tr9N4Cjq+rhHTHYncS7aM71/x5o\nfytwUfvzcprLm5fS/JZ/Nc2zarR9FtCc2/2B9cDf0oST3t0lnu+ZN7jOwXM+fL8KfB74t8BPgb8B\nDquqf2r7h3LO/bJASZLUOa5BkSRJnWNAkSRJnWNAkSRJnWNAkSRJnWNAkSRJnWNAkSRJnWNAkSRJ\nnWNAkSRJnWNAkSRJnWNAkSRJnWNAkSRJnfP/A21xkeu21wmBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f86ae232dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Word frequency distribution, just for kicks\n",
    "_=plt.hist(list(token_counts.values()),range=[0,50],bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Select only the tokens that had at least 10 occurences in the corpora.\n",
    "#Use token_counts.\n",
    "\n",
    "min_count = 10\n",
    "tokens = [token for token, count in token_counts.items() if count >= min_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_to_id = {t:i+1 for i,t in enumerate(tokens)}\n",
    "null_token = \"NULL\"\n",
    "token_to_id[null_token] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tokens: 84388\n"
     ]
    }
   ],
   "source": [
    "print(\"# Tokens:\",len(token_to_id))\n",
    "if len(token_to_id) < 30000:\n",
    "    print(\"Alarm! It seems like there are too few tokens. Make sure you updated NLTK and applied correct thresholds -- unless you now what you're doing, ofc\")\n",
    "if len(token_to_id) > 1000000:\n",
    "    print(\"Alarm! Too many tokens. You might have messed up when pruning rare ones -- unless you know what you're doin' ofc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace words with IDs\n",
    "Set a maximum length for titles and descriptions.\n",
    " * If string is longer that that limit - crop it, if less - pad with zeros.\n",
    " * Thus we obtain a matrix of size [n_samples]x[max_length]\n",
    " * Element at i,j - is an identifier of word j within sample i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(strings, token_to_id, max_len=150):\n",
    "    token_matrix = []\n",
    "    for s in strings:\n",
    "        if type(s) is not str:\n",
    "            token_matrix.append([0]*max_len)\n",
    "            continue\n",
    "        s = s.lower()\n",
    "        tokens = tokenizer.tokenize(s)\n",
    "        token_ids = list(map(lambda token: token_to_id.get(token,0), tokens))[:max_len]\n",
    "        token_ids += [0]*(max_len - len(token_ids))\n",
    "        token_matrix.append(token_ids)\n",
    "\n",
    "    return np.array(token_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "desc_tokens = vectorize(df.description.values,token_to_id,max_len = 150)\n",
    "title_tokens = vectorize(df.title.values,token_to_id,max_len = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Data format examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер матрицы: (500000, 15)\n",
      "Продам кальян новый 70 см -> [ 4977 56187  3004 28005 55136     0     0     0     0     0] ...\n",
      "Жир медвежий -> [45581  4748     0     0     0     0     0     0     0     0] ...\n",
      "Математика алгебра геометрия для школьников и студ -> [39621 34978 30897 59617 63914 81303 64773     0     0     0] ...\n"
     ]
    }
   ],
   "source": [
    "print(\"Размер матрицы:\",title_tokens.shape)\n",
    "for title, tokens in zip(df.title.values[:3],title_tokens[:3]):\n",
    "    print(title,'->', tokens[:10],'...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ As you can see, our preprocessing is somewhat crude. Let us see if that is enough for our network __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-sequences\n",
    "\n",
    "\n",
    "Some data features are not text samples. E.g. price, # urls, category, etc\n",
    "\n",
    "They require a separate preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#All numeric features\n",
    "df_numerical_features = df[[\"phones_cnt\",\"emails_cnt\",\"urls_cnt\",\"price\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#One-hot-encoded category and subcategory\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "categories = []\n",
    "data_cat_subcat = df[[\"category\",\"subcategory\"]].values\n",
    "\n",
    "categories = [{\"category\":category, \"subcategory\":subcategory} for category, subcategory in data_cat_subcat]\n",
    "\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "cat_one_hot = vectorizer.fit_transform(categories)\n",
    "cat_one_hot = pd.DataFrame(cat_one_hot,columns=vectorizer.feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_non_text = pd.merge(\n",
    "    df_numerical_features,cat_one_hot,on = np.arange(len(cat_one_hot))\n",
    ")\n",
    "del df_non_text[\"key_0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Target variable - whether or not sample contains prohibited material\n",
    "target = df.is_blocked.values.astype('int32')\n",
    "#Preprocessed titles\n",
    "title_tokens = title_tokens.astype('int32')\n",
    "#Preprocessed tokens\n",
    "desc_tokens = desc_tokens.astype('int32')\n",
    "\n",
    "#Non-sequences\n",
    "df_non_text = df_non_text.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Split into training and test set.\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "#Difficulty selector:\n",
    "#Easy: split randomly\n",
    "#Medium: select test set items that have item_ids strictly above that of training set\n",
    "#Hard: do whatever you want, but score yourself using kaggle private leaderboard\n",
    "\n",
    "title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts = train_test_split(title_tokens, desc_tokens, df_non_text, target, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save preprocessed data [optional]\n",
    "\n",
    "* The next tab can be used to stash all the essential data matrices and get rid of the rest of the data.\n",
    " * Highly recommended if you have less than 1.5GB RAM left\n",
    "* To do that, you need to first run it with save_prepared_data=True, then restart the notebook and only run this tab with read_prepared_data=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_prepared_data = True #save\n",
    "read_prepared_data = False #load\n",
    "\n",
    "#but not both at once\n",
    "assert not (save_prepared_data and read_prepared_data)\n",
    "\n",
    "if save_prepared_data:\n",
    "    print(\"Saving preprocessed data (may take up to 3 minutes)\")\n",
    "\n",
    "    import pickle\n",
    "    with open(\"preprocessed_data.pcl\",'w') as fout:\n",
    "        pickle.dump(data_tuple,fout)\n",
    "    with open(\"token_to_id.pcl\",'w') as fout:\n",
    "        pickle.dump(token_to_id,fout)\n",
    "\n",
    "    print(\"готово\")\n",
    "    \n",
    "elif read_prepared_data:\n",
    "    print(\"Reading saved data...\")\n",
    "    \n",
    "    import pickle\n",
    "    \n",
    "    with open(\"preprocessed_data.pcl\",'r') as fin:\n",
    "        data_tuple = pickle.load(fin)\n",
    "    title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts = data_tuple\n",
    "    with open(\"token_to_id.pcl\",'r') as fin:\n",
    "        token_to_id = pickle.load(fin)\n",
    "        \n",
    "    #Re-importing libraries to allow staring noteboook from here\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "   \n",
    "    print(\"done\")       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the monster\n",
    "\n",
    "Since we have several data sources, our neural network may differ from what you used to work with.\n",
    "\n",
    "* Separate input for titles: RNN\n",
    "* Separate input for description: RNN\n",
    "* Separate input for categorical features: обычные полносвязные слои или какие-нибудь трюки\n",
    " \n",
    "These three inputs must be blended somehow - concatenated or added.\n",
    "\n",
    "* Output: a simple binary classification\n",
    " * 1 sigmoidal with binary_crossentropy\n",
    " * 2 softmax with categorical_crossentropy - essentially the same as previous one\n",
    " * 1 neuron without nonlinearity (lambda x: x) +  hinge loss\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 1080 (CNMeM is enabled with initial size: 95.0% of memory, cuDNN 5105)\n"
     ]
    }
   ],
   "source": [
    "#libraries\n",
    "import lasagne\n",
    "from theano import tensor as T\n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3 inputs and a refere output\n",
    "title_token_ids = T.matrix(\"title_token_ids\",dtype='int32')\n",
    "desc_token_ids = T.matrix(\"desc_token_ids\",dtype='int32')\n",
    "categories = T.matrix(\"categories\",dtype='float32')\n",
    "target_y = T.ivector(\"is_blocked\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title_inp = lasagne.layers.InputLayer((None,title_tr.shape[1]),input_var=title_token_ids)\n",
    "descr_inp = lasagne.layers.InputLayer((None,desc_tr.shape[1]),input_var=desc_token_ids)\n",
    "cat_inp = lasagne.layers.InputLayer((None,nontext_tr.shape[1]), input_var=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Descriptions\n",
    "\n",
    "#word-wise embedding. We recommend to start from some 64 and improving after you are certain it works.\n",
    "descr_nn = lasagne.layers.EmbeddingLayer(descr_inp, input_size=len(token_to_id)+1, output_size=512)\n",
    "descr_nn = lasagne.layers.LSTMLayer(descr_nn, num_units=512)\n",
    "descr_nn = lasagne.layers.LSTMLayer(descr_nn, num_units=512)\n",
    "descr_nn = lasagne.layers.LSTMLayer(descr_nn, num_units=512)\n",
    "descr_nn = lasagne.layers.flatten(descr_nn)\n",
    "\n",
    "# Titles\n",
    "title_nn = lasagne.layers.EmbeddingLayer(title_inp, input_size=len(token_to_id) + 1, output_size=64)\n",
    "title_nn = lasagne.layers.LSTMLayer(title_nn, num_units=128)\n",
    "title_nn = lasagne.layers.LSTMLayer(title_nn, num_units=128)\n",
    "title_nn = lasagne.layers.LSTMLayer(title_nn, num_units=128)\n",
    "title_nn = lasagne.layers.flatten(title_nn)\n",
    "# Non-sequences\n",
    "cat_nn = lasagne.layers.DenseLayer(cat_inp, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn = lasagne.layers.concat([descr_nn, title_nn, cat_nn])                                 \n",
    "nn = lasagne.layers.DenseLayer(nn, 512)\n",
    "nn = lasagne.layers.DropoutLayer(nn,p=0.5)\n",
    "nn = lasagne.layers.DenseLayer(nn,1,nonlinearity=lasagne.nonlinearities.linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function\n",
    "\n",
    "* The standard way:\n",
    " * prediction\n",
    " * loss\n",
    " * updates\n",
    " * training and evaluation functions\n",
    " \n",
    " \n",
    "* Hinge loss\n",
    " * $ L_i = \\max(0, \\delta - t_i p_i) $\n",
    " * delta is a tunable parameter: how far should a neuron be in the positive margin area for us to stop bothering about it\n",
    " * Function description may mention some +-1  limitations - this is not neccessary, at least as long as hinge loss has a __default__ flag `binary = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#All trainable params\n",
    "weights = lasagne.layers.get_all_params(nn,trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Simple NN prediction\n",
    "prediction = lasagne.layers.get_output(nn)[:,0]\n",
    "\n",
    "#Hinge loss\n",
    "loss = lasagne.objectives.binary_hinge_loss(prediction,target_y,delta = 1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Weight optimization step\n",
    "updates = lasagne.updates.adam(loss, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determinitic prediction \n",
    " * In case we use stochastic elements, e.g. dropout or noize\n",
    " * Compile a separate set of functions with deterministic prediction (deterministic = True)\n",
    " * Unless you think there's no neet for dropout there ofc. Btw is there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#deterministic version\n",
    "det_prediction = lasagne.layers.get_output(nn,deterministic=True)[:,0]\n",
    "\n",
    "#equivalent loss function\n",
    "det_loss = lasagne.objectives.binary_hinge_loss(det_prediction,target_y,delta = 1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coffee-lation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_fun = theano.function([desc_token_ids,title_token_ids,categories,target_y],[loss,prediction],updates = updates)\n",
    "eval_fun = theano.function([desc_token_ids,title_token_ids,categories,target_y],[det_loss,det_prediction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "* The regular way with loops over minibatches\n",
    "* Since the dataset is huge, we define epoch as some fixed amount of samples isntead of all dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#average precision at K\n",
    "\n",
    "from oracle import APatK, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Out good old minibatch iterator now supports arbitrary amount of arrays (X,y,z)\n",
    "\n",
    "def iterate_minibatches(*arrays,**kwargs):\n",
    "    batchsize=kwargs.get(\"batchsize\",100)\n",
    "    shuffle = kwargs.get(\"shuffle\",True)\n",
    "    \n",
    "    if shuffle:\n",
    "        indices = np.arange(len(arrays[0]))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(arrays[0]) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield [arr[excerpt] for arr in arrays]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweaking guide\n",
    "\n",
    "* batch_size - how many samples are processed per function call\n",
    "  * optimization gets slower, but more stable, as you increase it.\n",
    "  * May consider increasing it halfway through training\n",
    "* minibatches_per_epoch - max amount of minibatches per epoch\n",
    "  * Does not affect training. Lesser value means more frequent and less stable printing\n",
    "  * Setting it to less than 10 is only meaningfull if you want to make sure your NN does not break down after one epoch\n",
    "* n_epochs - total amount of epochs to train for\n",
    "  * `n_epochs = 10**10` and manual interrupting is still an option\n",
    "\n",
    "\n",
    "Tips:\n",
    "\n",
    "* With small minibatches_per_epoch, network quality may jump around 0.5 for several epochs\n",
    "\n",
    "* AUC is the most stable of all three metrics\n",
    "\n",
    "* Average Precision at top 2.5% (APatK) - is the least stable. If batch_size*minibatches_per_epoch < 10k, it behaves as a uniform random variable.\n",
    "\n",
    "* Plotting metrics over training time may be a good way to analyze which architectures work better.\n",
    "\n",
    "* Once you are sure your network aint gonna crash, it's worth letting it train for a few hours of an average laptop's time to see it's true potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "\tloss: 8540.0366757\n",
      "\tacc: 0.636534653465\n",
      "\tauc: 0.645811845241\n",
      "\tap@k: 0.0903227436192\n",
      "Val:\n",
      "\tloss: 2306.04049241\n",
      "\tacc: 0.709603960396\n",
      "\tauc: 0.691912327262\n",
      "\tap@k: 0.932825674204\n",
      "Train:\n",
      "\tloss: 2007.87977994\n",
      "\tacc: 0.738910891089\n",
      "\tauc: 0.749239378462\n",
      "\tap@k: 0.0964250603546\n",
      "Val:\n",
      "\tloss: 10144.404946\n",
      "\tacc: 0.747425742574\n",
      "\tauc: 0.733868354724\n",
      "\tap@k: 0.875326729113\n",
      "Train:\n",
      "\tloss: 1899.49950954\n",
      "\tacc: 0.768415841584\n",
      "\tauc: 0.813682494147\n",
      "\tap@k: 0.120238125635\n",
      "Val:\n",
      "\tloss: 6825.97466328\n",
      "\tacc: 0.766138613861\n",
      "\tauc: 0.730978099013\n",
      "\tap@k: 0.401202734582\n",
      "Train:\n",
      "\tloss: 2793.43059289\n",
      "\tacc: 0.793663366337\n",
      "\tauc: 0.834971706232\n",
      "\tap@k: 0.143937270894\n",
      "Val:\n",
      "\tloss: 603.037255793\n",
      "\tacc: 0.814455445545\n",
      "\tauc: 0.838447264475\n",
      "\tap@k: 0.612287510411\n",
      "Train:\n",
      "\tloss: 2112.80781006\n",
      "\tacc: 0.797227722772\n",
      "\tauc: 0.813854343176\n",
      "\tap@k: 0.0696820427547\n",
      "Val:\n",
      "\tloss: 3215.00392208\n",
      "\tacc: 0.787722772277\n",
      "\tauc: 0.789808107578\n",
      "\tap@k: 0.47033849359\n",
      "Train:\n",
      "\tloss: 1078.73414999\n",
      "\tacc: 0.784554455446\n",
      "\tauc: 0.835080951519\n",
      "\tap@k: 0.249288425644\n",
      "Val:\n",
      "\tloss: 937.114086707\n",
      "\tacc: 0.773465346535\n",
      "\tauc: 0.804865975625\n",
      "\tap@k: 0.695796337091\n",
      "Train:\n",
      "\tloss: 716.650062639\n",
      "\tacc: 0.771683168317\n",
      "\tauc: 0.816523144492\n",
      "\tap@k: 0.439664111863\n",
      "Val:\n",
      "\tloss: 1809.33390128\n",
      "\tacc: 0.781683168317\n",
      "\tauc: 0.830183089573\n",
      "\tap@k: 0.920170890954\n",
      "Train:\n",
      "\tloss: 586.112215743\n",
      "\tacc: 0.750396039604\n",
      "\tauc: 0.805576866487\n",
      "\tap@k: 0.349734486511\n",
      "Val:\n",
      "\tloss: 51.9135520211\n",
      "\tacc: 0.750594059406\n",
      "\tauc: 0.890123048496\n",
      "\tap@k: 0.911436927444\n",
      "Train:\n",
      "\tloss: 413.882814218\n",
      "\tacc: 0.768118811881\n",
      "\tauc: 0.800701214954\n",
      "\tap@k: 0.113528721523\n",
      "Val:\n",
      "\tloss: 144.023778794\n",
      "\tacc: 0.761881188119\n",
      "\tauc: 0.843061841307\n",
      "\tap@k: 0.853874035917\n",
      "Train:\n",
      "\tloss: 61.0979067815\n",
      "\tacc: 0.76702970297\n",
      "\tauc: 0.816841589245\n",
      "\tap@k: 0.584210225706\n",
      "Val:\n",
      "\tloss: 147.484641864\n",
      "\tacc: 0.719306930693\n",
      "\tauc: 0.85716568686\n",
      "\tap@k: 0.662747146444\n",
      "Train:\n",
      "\tloss: 61.0338472984\n",
      "\tacc: 0.698712871287\n",
      "\tauc: 0.772710717055\n",
      "\tap@k: 0.303307503651\n",
      "Val:\n",
      "\tloss: 392.398978679\n",
      "\tacc: 0.720396039604\n",
      "\tauc: 0.743301886685\n",
      "\tap@k: 0.671867147219\n",
      "Train:\n",
      "\tloss: 463.51414017\n",
      "\tacc: 0.734851485149\n",
      "\tauc: 0.735165397527\n",
      "\tap@k: 0.309066247763\n",
      "Val:\n",
      "\tloss: 33.4831591102\n",
      "\tacc: 0.673861386139\n",
      "\tauc: 0.681915919132\n",
      "\tap@k: 0.78126521967\n",
      "Train:\n",
      "\tloss: 53.7749584751\n",
      "\tacc: 0.667227722772\n",
      "\tauc: 0.685870561582\n",
      "\tap@k: 0.452372341491\n",
      "Val:\n",
      "\tloss: 61.9001892198\n",
      "\tacc: 0.658712871287\n",
      "\tauc: 0.667396793905\n",
      "\tap@k: 0.739363517637\n",
      "Train:\n",
      "\tloss: 299.129164339\n",
      "\tacc: 0.725148514851\n",
      "\tauc: 0.741209249495\n",
      "\tap@k: 0.388300705039\n",
      "Val:\n",
      "\tloss: 47.3909398109\n",
      "\tacc: 0.755940594059\n",
      "\tauc: 0.770496252553\n",
      "\tap@k: 0.787176392791\n",
      "Train:\n",
      "\tloss: 47.3407333874\n",
      "\tacc: 0.743069306931\n",
      "\tauc: 0.741663152302\n",
      "\tap@k: 0.481735179487\n",
      "Val:\n",
      "\tloss: 23.4632386313\n",
      "\tacc: 0.756237623762\n",
      "\tauc: 0.768151549107\n",
      "\tap@k: 0.762222636607\n",
      "Train:\n",
      "\tloss: 20.5797226272\n",
      "\tacc: 0.736534653465\n",
      "\tauc: 0.750929642354\n",
      "\tap@k: 0.556100238539\n",
      "Val:\n",
      "\tloss: 5.61017377489\n",
      "\tacc: 0.772871287129\n",
      "\tauc: 0.788227129739\n",
      "\tap@k: 0.896147604349\n",
      "Train:\n",
      "\tloss: 6.32755987635\n",
      "\tacc: 0.758514851485\n",
      "\tauc: 0.775476877335\n",
      "\tap@k: 0.598873357878\n",
      "Val:\n",
      "\tloss: 27.6230946091\n",
      "\tacc: 0.763861386139\n",
      "\tauc: 0.776499983374\n",
      "\tap@k: 0.822053412513\n",
      "Train:\n",
      "\tloss: 11.7324362513\n",
      "\tacc: 0.726930693069\n",
      "\tauc: 0.734815410487\n",
      "\tap@k: 0.617181087243\n",
      "Val:\n",
      "\tloss: 13.3457391716\n",
      "\tacc: 0.79\n",
      "\tauc: 0.805509150646\n",
      "\tap@k: 0.859034836864\n",
      "Train:\n",
      "\tloss: 12.3784439488\n",
      "\tacc: 0.74198019802\n",
      "\tauc: 0.754664620183\n",
      "\tap@k: 0.669706941401\n",
      "Val:\n",
      "\tloss: 3.63178542627\n",
      "\tacc: 0.730396039604\n",
      "\tauc: 0.746322796916\n",
      "\tap@k: 0.903486963516\n",
      "Train:\n",
      "\tloss: 8.87081649597\n",
      "\tacc: 0.702376237624\n",
      "\tauc: 0.704274426961\n",
      "\tap@k: 0.612688696667\n",
      "Val:\n",
      "\tloss: 8.60554494914\n",
      "\tacc: 0.695841584158\n",
      "\tauc: 0.709428478439\n",
      "\tap@k: 0.886088098668\n",
      "Train:\n",
      "\tloss: 27.7727615714\n",
      "\tacc: 0.689702970297\n",
      "\tauc: 0.701947841256\n",
      "\tap@k: 0.568792350694\n",
      "Val:\n",
      "\tloss: 7.51580189915\n",
      "\tacc: 0.790495049505\n",
      "\tauc: 0.799417014092\n",
      "\tap@k: 0.828143729922\n",
      "Train:\n",
      "\tloss: 3.53055565514\n",
      "\tacc: 0.734554455446\n",
      "\tauc: 0.746752288647\n",
      "\tap@k: 0.635038375166\n",
      "Val:\n",
      "\tloss: 5.6833389127\n",
      "\tacc: 0.77504950495\n",
      "\tauc: 0.787975997419\n",
      "\tap@k: 0.815836958763\n",
      "Train:\n",
      "\tloss: 2.42463961422\n",
      "\tacc: 0.739702970297\n",
      "\tauc: 0.739212221219\n",
      "\tap@k: 0.667419106657\n",
      "Val:\n",
      "\tloss: 5.19525411231\n",
      "\tacc: 0.759801980198\n",
      "\tauc: 0.769210119205\n",
      "\tap@k: 0.813586060966\n",
      "Train:\n",
      "\tloss: 12.3529089725\n",
      "\tacc: 0.769603960396\n",
      "\tauc: 0.768518057565\n",
      "\tap@k: 0.666893827949\n",
      "Val:\n",
      "\tloss: 3.63715804958\n",
      "\tacc: 0.778811881188\n",
      "\tauc: 0.789681591122\n",
      "\tap@k: 0.777063753353\n",
      "Train:\n",
      "\tloss: 4.50582745482\n",
      "\tacc: 0.742574257426\n",
      "\tauc: 0.747862129309\n",
      "\tap@k: 0.70367034495\n",
      "Val:\n",
      "\tloss: 5.25519944953\n",
      "\tacc: 0.767722772277\n",
      "\tauc: 0.776086452561\n",
      "\tap@k: 0.790540230487\n",
      "Train:\n",
      "\tloss: 3.67456857343\n",
      "\tacc: 0.748613861386\n",
      "\tauc: 0.773624663166\n",
      "\tap@k: 0.664245209083\n",
      "Val:\n",
      "\tloss: 3.11386451575\n",
      "\tacc: 0.821683168317\n",
      "\tauc: 0.864453576121\n",
      "\tap@k: 0.85630259545\n",
      "Train:\n",
      "\tloss: 1.81170859813\n",
      "\tacc: 0.793564356436\n",
      "\tauc: 0.819114219078\n",
      "\tap@k: 0.620010267277\n",
      "Val:\n",
      "\tloss: 1.05262792047\n",
      "\tacc: 0.872772277228\n",
      "\tauc: 0.909496848498\n",
      "\tap@k: 0.848535869004\n",
      "Train:\n",
      "\tloss: 1.90537430015\n",
      "\tacc: 0.833465346535\n",
      "\tauc: 0.860500710146\n",
      "\tap@k: 0.783320526503\n",
      "Val:\n",
      "\tloss: 0.513989383325\n",
      "\tacc: 0.868514851485\n",
      "\tauc: 0.912513911274\n",
      "\tap@k: 0.86427398293\n",
      "Train:\n",
      "\tloss: 3.29111300211\n",
      "\tacc: 0.832673267327\n",
      "\tauc: 0.856718601775\n",
      "\tap@k: 0.768542993064\n",
      "Val:\n",
      "\tloss: 1.45377118403\n",
      "\tacc: 0.845742574257\n",
      "\tauc: 0.911348390127\n",
      "\tap@k: 0.867133512286\n",
      "Train:\n",
      "\tloss: 1.27652012677\n",
      "\tacc: 0.828811881188\n",
      "\tauc: 0.855277803527\n",
      "\tap@k: 0.780950708924\n",
      "Val:\n",
      "\tloss: 0.311474802169\n",
      "\tacc: 0.870495049505\n",
      "\tauc: 0.902613916766\n",
      "\tap@k: 0.879730395975\n",
      "Train:\n",
      "\tloss: 2.88526758592\n",
      "\tacc: 0.835346534653\n",
      "\tauc: 0.857262014444\n",
      "\tap@k: 0.76076030644\n",
      "Val:\n",
      "\tloss: 0.334111747571\n",
      "\tacc: 0.870594059406\n",
      "\tauc: 0.911441438396\n",
      "\tap@k: 0.806832538447\n",
      "Train:\n",
      "\tloss: 0.868780675501\n",
      "\tacc: 0.836237623762\n",
      "\tauc: 0.863699186939\n",
      "\tap@k: 0.711492324037\n",
      "Val:\n",
      "\tloss: 4.35026017131\n",
      "\tacc: 0.865148514851\n",
      "\tauc: 0.902432505548\n",
      "\tap@k: 0.920545157439\n",
      "Train:\n",
      "\tloss: 1.8044037108\n",
      "\tacc: 0.826237623762\n",
      "\tauc: 0.849790691769\n",
      "\tap@k: 0.813528543941\n",
      "Val:\n",
      "\tloss: 1.43550285467\n",
      "\tacc: 0.882871287129\n",
      "\tauc: 0.923897770406\n",
      "\tap@k: 0.836993009657\n",
      "Train:\n",
      "\tloss: 1.43115881709\n",
      "\tacc: 0.832574257426\n",
      "\tauc: 0.861062630257\n",
      "\tap@k: 0.91234083752\n",
      "Val:\n",
      "\tloss: 0.287023883928\n",
      "\tacc: 0.879207920792\n",
      "\tauc: 0.918160056859\n",
      "\tap@k: 0.916733933594\n",
      "Train:\n",
      "\tloss: 0.381893862209\n",
      "\tacc: 0.83099009901\n",
      "\tauc: 0.857320529886\n",
      "\tap@k: 0.828308886022\n",
      "Val:\n",
      "\tloss: 0.280932537134\n",
      "\tacc: 0.887821782178\n",
      "\tauc: 0.928694377784\n",
      "\tap@k: 0.912068551399\n",
      "Train:\n",
      "\tloss: 0.568834226674\n",
      "\tacc: 0.842079207921\n",
      "\tauc: 0.870721663248\n",
      "\tap@k: 0.767053111837\n",
      "Val:\n",
      "\tloss: 0.298624130605\n",
      "\tacc: 0.880891089109\n",
      "\tauc: 0.928655240774\n",
      "\tap@k: 0.880956935588\n",
      "Train:\n",
      "\tloss: 0.666067153865\n",
      "\tacc: 0.852475247525\n",
      "\tauc: 0.876641147777\n",
      "\tap@k: 0.536430191705\n",
      "Val:\n",
      "\tloss: 0.272528829097\n",
      "\tacc: 0.88504950495\n",
      "\tauc: 0.918315498668\n",
      "\tap@k: 0.862472129562\n",
      "Val:\n",
      "\tloss: 0.222567380065\n",
      "\tacc: 0.906831683168\n",
      "\tauc: 0.932170468314\n",
      "\tap@k: 0.951021957798\n",
      "Train:\n",
      "\tloss: 0.271457916126\n",
      "\tacc: 0.885148514851\n",
      "\tauc: 0.906040153845\n",
      "\tap@k: 0.850246818326\n",
      "Val:\n",
      "\tloss: 0.222089728235\n",
      "\tacc: 0.911485148515\n",
      "\tauc: 0.938754267549\n",
      "\tap@k: 0.877104131445\n",
      "Train:\n",
      "\tloss: 0.269348115148\n",
      "\tacc: 0.881287128713\n",
      "\tauc: 0.909297640682\n",
      "\tap@k: 0.84896769908\n",
      "Val:\n",
      "\tloss: 0.232869033483\n",
      "\tacc: 0.904257425743\n",
      "\tauc: 0.939279230086\n",
      "\tap@k: 0.961429291656\n",
      "Train:\n",
      "\tloss: 0.315307067202\n",
      "\tacc: 0.885742574257\n",
      "\tauc: 0.904864460529\n",
      "\tap@k: 0.894334992016\n",
      "Val:\n",
      "\tloss: 0.219679456743\n",
      "\tacc: 0.905742574257\n",
      "\tauc: 0.939229279157\n",
      "\tap@k: 0.872312617752\n",
      "Train:\n",
      "\tloss: 0.26761529304\n",
      "\tacc: 0.885643564356\n",
      "\tauc: 0.910962639209\n",
      "\tap@k: 0.898177038753\n",
      "Val:\n",
      "\tloss: 0.201761235695\n",
      "\tacc: 0.914653465347\n",
      "\tauc: 0.950813838533\n",
      "\tap@k: 0.915194378587\n",
      "Train:\n",
      "\tloss: 0.250747500044\n",
      "\tacc: 0.894059405941\n",
      "\tauc: 0.919743696104\n",
      "\tap@k: 0.840715989255\n",
      "Val:\n",
      "\tloss: 0.232543023122\n",
      "\tacc: 0.915247524752\n",
      "\tauc: 0.946001396966\n",
      "\tap@k: 0.906319050936\n",
      "Train:\n",
      "\tloss: 0.23194175604\n",
      "\tacc: 0.90603960396\n",
      "\tauc: 0.928788664936\n",
      "\tap@k: 0.865603321667\n",
      "Val:\n",
      "\tloss: 0.215256864753\n",
      "\tacc: 0.922574257426\n",
      "\tauc: 0.953532777572\n",
      "\tap@k: 0.952308724067\n",
      "Train:\n",
      "\tloss: 0.241770020464\n",
      "\tacc: 0.898811881188\n",
      "\tauc: 0.922273519104\n",
      "\tap@k: 0.88626149962\n",
      "Val:\n",
      "\tloss: 0.190887990069\n",
      "\tacc: 0.921386138614\n",
      "\tauc: 0.954561173251\n",
      "\tap@k: 0.941419740394\n",
      "Train:\n",
      "\tloss: 0.2493032645\n",
      "\tacc: 0.893861386139\n",
      "\tauc: 0.918219584819\n",
      "\tap@k: 0.900731651088\n",
      "Val:\n",
      "\tloss: 0.17511586574\n",
      "\tacc: 0.928316831683\n",
      "\tauc: 0.961256044194\n",
      "\tap@k: 0.975501840063\n",
      "Train:\n",
      "\tloss: 0.440761214524\n",
      "\tacc: 0.889900990099\n",
      "\tauc: 0.920177599267\n",
      "\tap@k: 0.791061080951\n",
      "Val:\n",
      "\tloss: 0.202107706766\n",
      "\tacc: 0.925346534653\n",
      "\tauc: 0.961460056383\n",
      "\tap@k: 0.92227081055\n",
      "Train:\n",
      "\tloss: 0.232897488256\n",
      "\tacc: 0.90297029703\n",
      "\tauc: 0.932604410412\n",
      "\tap@k: 0.857215064303\n",
      "Val:\n",
      "\tloss: 0.196422239217\n",
      "\tacc: 0.91603960396\n",
      "\tauc: 0.95583477403\n",
      "\tap@k: 0.962075110657\n",
      "Train:\n",
      "\tloss: 0.219636338701\n",
      "\tacc: 0.904653465347\n",
      "\tauc: 0.929115970339\n",
      "\tap@k: 0.830850402407\n",
      "Val:\n",
      "\tloss: 0.310037441761\n",
      "\tacc: 0.840495049505\n",
      "\tauc: 0.957071839674\n",
      "\tap@k: 0.846362555557\n",
      "Train:\n",
      "\tloss: 0.199501086438\n",
      "\tacc: 0.91801980198\n",
      "\tauc: 0.937339709935\n",
      "\tap@k: 0.651184121219\n",
      "Val:\n",
      "\tloss: 0.213386694517\n",
      "\tacc: 0.932079207921\n",
      "\tauc: 0.964167444674\n",
      "\tap@k: 0.941450111943\n",
      "Train:\n",
      "\tloss: 0.21530294223\n",
      "\tacc: 0.907821782178\n",
      "\tauc: 0.936409074419\n",
      "\tap@k: 0.836025468325\n",
      "Val:\n",
      "\tloss: 0.181723552039\n",
      "\tacc: 0.922871287129\n",
      "\tauc: 0.963685703333\n",
      "\tap@k: 0.911878759014\n",
      "Train:\n",
      "\tloss: 0.205852136588\n",
      "\tacc: 0.913465346535\n",
      "\tauc: 0.942899080605\n",
      "\tap@k: 0.919387129961\n",
      "Val:\n",
      "\tloss: 0.194826168501\n",
      "\tacc: 0.927128712871\n",
      "\tauc: 0.965866887327\n",
      "\tap@k: 0.972857531793\n",
      "Train:\n",
      "\tloss: 0.218169679353\n",
      "\tacc: 0.909207920792\n",
      "\tauc: 0.934754570755\n",
      "\tap@k: 0.894700861062\n",
      "Val:\n",
      "\tloss: 0.175918518789\n",
      "\tacc: 0.935940594059\n",
      "\tauc: 0.967967740133\n",
      "\tap@k: 0.95843630651\n",
      "Train:\n",
      "\tloss: 0.218390098214\n",
      "\tacc: 0.91297029703\n",
      "\tauc: 0.939725347535\n",
      "\tap@k: 0.907730532842\n",
      "Val:\n",
      "\tloss: 0.278312303483\n",
      "\tacc: 0.93495049505\n",
      "\tauc: 0.972629501678\n",
      "\tap@k: 0.983641000211\n",
      "Train:\n",
      "\tloss: 0.211454796118\n",
      "\tacc: 0.916138613861\n",
      "\tauc: 0.943109415116\n",
      "\tap@k: 0.933226748745\n",
      "Val:\n",
      "\tloss: 0.147471203887\n",
      "\tacc: 0.940891089109\n",
      "\tauc: 0.973567639675\n",
      "\tap@k: 0.947928878533\n",
      "Train:\n",
      "\tloss: 0.173277895055\n",
      "\tacc: 0.922871287129\n",
      "\tauc: 0.945753800755\n",
      "\tap@k: 0.941148290636\n",
      "Val:\n",
      "\tloss: 0.165180503496\n",
      "\tacc: 0.941881188119\n",
      "\tauc: 0.971589534598\n",
      "\tap@k: 0.974702106376\n",
      "Train:\n",
      "\tloss: 0.188615644173\n",
      "\tacc: 0.925742574257\n",
      "\tauc: 0.949057678745\n",
      "\tap@k: 0.900721984042\n",
      "Val:\n",
      "\tloss: 0.18755378251\n",
      "\tacc: 0.933267326733\n",
      "\tauc: 0.966803045503\n",
      "\tap@k: 0.949185669561\n",
      "Train:\n",
      "\tloss: 5.56063657022\n",
      "\tacc: 0.91603960396\n",
      "\tauc: 0.939944632011\n",
      "\tap@k: 0.872297132077\n",
      "Val:\n",
      "\tloss: 2.56695611034\n",
      "\tacc: 0.91702970297\n",
      "\tauc: 0.966085080157\n",
      "\tap@k: 0.985010252706\n",
      "Train:\n",
      "\tloss: 0.294172501723\n",
      "\tacc: 0.884158415842\n",
      "\tauc: 0.904120203277\n",
      "\tap@k: 0.580726846091\n",
      "Val:\n",
      "\tloss: 0.196312096467\n",
      "\tacc: 0.928811881188\n",
      "\tauc: 0.962913176471\n",
      "\tap@k: 0.959208523722\n",
      "Train:\n",
      "\tloss: 0.187911847145\n",
      "\tacc: 0.919108910891\n",
      "\tauc: 0.94162452333\n",
      "\tap@k: 0.925625347581\n",
      "Val:\n",
      "\tloss: 0.175423170116\n",
      "\tacc: 0.931089108911\n",
      "\tauc: 0.965924942211\n",
      "\tap@k: 0.938997553226\n",
      "Train:\n",
      "\tloss: 0.178622792033\n",
      "\tacc: 0.925841584158\n",
      "\tauc: 0.948278770903\n",
      "\tap@k: 0.903108510654\n",
      "Val:\n",
      "\tloss: 0.165616700124\n",
      "\tacc: 0.93495049505\n",
      "\tauc: 0.967008275821\n",
      "\tap@k: 0.944774363421\n",
      "Train:\n",
      "\tloss: 0.26567186129\n",
      "\tacc: 0.887821782178\n",
      "\tauc: 0.908578324173\n",
      "\tap@k: 0.838729956697\n",
      "Val:\n",
      "\tloss: 0.132273957629\n",
      "\tacc: 0.944653465347\n",
      "\tauc: 0.97517352514\n",
      "\tap@k: 0.968426648657\n",
      "Train:\n",
      "\tloss: 0.217844006219\n",
      "\tacc: 0.90198019802\n",
      "\tauc: 0.923563797971\n",
      "\tap@k: 0.927535603923\n",
      "Val:\n",
      "\tloss: 0.180281387219\n",
      "\tacc: 0.940891089109\n",
      "\tauc: 0.971236092537\n",
      "\tap@k: 0.961742246923\n",
      "Train:\n",
      "\tloss: 0.18775277853\n",
      "\tacc: 0.923861386139\n",
      "\tauc: 0.941599212669\n",
      "\tap@k: 0.924420702002\n",
      "Val:\n",
      "\tloss: 0.1527351748\n",
      "\tacc: 0.935742574257\n",
      "\tauc: 0.970584669121\n",
      "\tap@k: 0.958299842759\n",
      "Train:\n",
      "\tloss: 0.188453435638\n",
      "\tacc: 0.920594059406\n",
      "\tauc: 0.940222782649\n",
      "\tap@k: 0.896532330033\n",
      "Val:\n",
      "\tloss: 0.147329603286\n",
      "\tacc: 0.944554455446\n",
      "\tauc: 0.973800504073\n",
      "\tap@k: 0.956119304572\n",
      "Train:\n",
      "\tloss: 0.196713313384\n",
      "\tacc: 0.92297029703\n",
      "\tauc: 0.946617555302\n",
      "\tap@k: 0.910199255206\n",
      "Val:\n",
      "\tloss: 0.552329309565\n",
      "\tacc: 0.915940594059\n",
      "\tauc: 0.942223726366\n",
      "\tap@k: 0.888805538046\n",
      "Train:\n",
      "\tloss: 0.190302241681\n",
      "\tacc: 0.924257425743\n",
      "\tauc: 0.945971971641\n",
      "\tap@k: 0.872581974169\n",
      "Val:\n",
      "\tloss: 0.202218458847\n",
      "\tacc: 0.943564356436\n",
      "\tauc: 0.972560231679\n",
      "\tap@k: 0.969740949868\n",
      "Train:\n",
      "\tloss: 0.161836148642\n",
      "\tacc: 0.935148514851\n",
      "\tauc: 0.957247151569\n",
      "\tap@k: 0.937932775985\n",
      "Val:\n",
      "\tloss: 0.164852511499\n",
      "\tacc: 0.94504950495\n",
      "\tauc: 0.972113310904\n",
      "\tap@k: 0.938386354805\n",
      "Train:\n",
      "\tloss: 0.164988115684\n",
      "\tacc: 0.934059405941\n",
      "\tauc: 0.955579557396\n",
      "\tap@k: 0.92000295432\n",
      "Val:\n",
      "\tloss: 0.146903260805\n",
      "\tacc: 0.94702970297\n",
      "\tauc: 0.97690177497\n",
      "\tap@k: 0.968601293763\n",
      "Train:\n",
      "\tloss: 0.45491747266\n",
      "\tacc: 0.919504950495\n",
      "\tauc: 0.936910110626\n",
      "\tap@k: 0.868823803433\n",
      "Val:\n",
      "\tloss: 0.180299749492\n",
      "\tacc: 0.925346534653\n",
      "\tauc: 0.958637742932\n",
      "\tap@k: 0.946354696286\n",
      "Train:\n",
      "\tloss: 0.190125539067\n",
      "\tacc: 0.918514851485\n",
      "\tauc: 0.938847159847\n",
      "\tap@k: 0.880005720618\n",
      "Val:\n",
      "\tloss: 0.177704878166\n",
      "\tacc: 0.921386138614\n",
      "\tauc: 0.962852599858\n",
      "\tap@k: 0.958629664301\n",
      "Train:\n",
      "\tloss: 0.162495577648\n",
      "\tacc: 0.930693069307\n",
      "\tauc: 0.949644494167\n",
      "\tap@k: 0.939209793591\n",
      "Val:\n",
      "\tloss: 0.130264297441\n",
      "\tacc: 0.945940594059\n",
      "\tauc: 0.975447683421\n",
      "\tap@k: 0.931985665284\n",
      "Train:\n",
      "\tloss: 0.171746056853\n",
      "\tacc: 0.931782178218\n",
      "\tauc: 0.950090348627\n",
      "\tap@k: 0.807052401682\n",
      "Val:\n",
      "\tloss: 0.149531202659\n",
      "\tacc: 0.942376237624\n",
      "\tauc: 0.972085406394\n",
      "\tap@k: 0.97539846478\n",
      "Train:\n",
      "\tloss: 0.173041789514\n",
      "\tacc: 0.928811881188\n",
      "\tauc: 0.949349620874\n",
      "\tap@k: 0.933719450187\n",
      "Val:\n",
      "\tloss: 0.150439831833\n",
      "\tacc: 0.951089108911\n",
      "\tauc: 0.981579702195\n",
      "\tap@k: 0.981103974435\n",
      "Train:\n",
      "\tloss: 0.166632529468\n",
      "\tacc: 0.937227722772\n",
      "\tauc: 0.957186424007\n",
      "\tap@k: 0.897280520995\n",
      "Val:\n",
      "\tloss: 0.182690619431\n",
      "\tacc: 0.92495049505\n",
      "\tauc: 0.960932882596\n",
      "\tap@k: 0.947993195211\n",
      "Train:\n",
      "\tloss: 0.161657519754\n",
      "\tacc: 0.93801980198\n",
      "\tauc: 0.959315703672\n",
      "\tap@k: 0.901703076744\n",
      "Val:\n",
      "\tloss: 0.122629983632\n",
      "\tacc: 0.951782178218\n",
      "\tauc: 0.975217684593\n",
      "\tap@k: 0.981727271171\n",
      "Train:\n",
      "\tloss: 0.142714285373\n",
      "\tacc: 0.937821782178\n",
      "\tauc: 0.956811764929\n",
      "\tap@k: 0.924853315406\n",
      "Val:\n",
      "\tloss: 0.183961567005\n",
      "\tacc: 0.95099009901\n",
      "\tauc: 0.973948748199\n",
      "\tap@k: 0.949765080635\n",
      "Train:\n",
      "\tloss: 0.146540291557\n",
      "\tacc: 0.939801980198\n",
      "\tauc: 0.957024301986\n",
      "\tap@k: 0.937469398702\n",
      "Val:\n",
      "\tloss: 0.116765365164\n",
      "\tacc: 0.955841584158\n",
      "\tauc: 0.975637586717\n",
      "\tap@k: 0.977849673827\n",
      "Train:\n",
      "\tloss: 0.142699398617\n",
      "\tacc: 0.941188118812\n",
      "\tauc: 0.958451131167\n",
      "\tap@k: 0.937811308313\n",
      "Val:\n",
      "\tloss: 0.176511488427\n",
      "\tacc: 0.950297029703\n",
      "\tauc: 0.975977221947\n",
      "\tap@k: 0.946435760086\n",
      "Train:\n",
      "\tloss: 0.118158479551\n",
      "\tacc: 0.949801980198\n",
      "\tauc: 0.96710886297\n",
      "\tap@k: 0.958391989494\n",
      "Val:\n",
      "\tloss: 0.159840654701\n",
      "\tacc: 0.95099009901\n",
      "\tauc: 0.976558572689\n",
      "\tap@k: 0.964870536559\n",
      "Train:\n",
      "\tloss: 0.136502743523\n",
      "\tacc: 0.941683168317\n",
      "\tauc: 0.959242231602\n",
      "\tap@k: 0.90461927533\n",
      "Val:\n",
      "\tloss: 0.146936687001\n",
      "\tacc: 0.947623762376\n",
      "\tauc: 0.970729390904\n",
      "\tap@k: 0.924360863832\n",
      "Train:\n",
      "\tloss: 0.143351143752\n",
      "\tacc: 0.942673267327\n",
      "\tauc: 0.955211619472\n",
      "\tap@k: 0.891735559114\n",
      "Val:\n",
      "\tloss: 0.130075716736\n",
      "\tacc: 0.94900990099\n",
      "\tauc: 0.972225968149\n",
      "\tap@k: 0.948895218691\n",
      "Train:\n",
      "\tloss: 0.147919266637\n",
      "\tacc: 0.939702970297\n",
      "\tauc: 0.958880665399\n",
      "\tap@k: 0.923744633864\n",
      "Val:\n",
      "\tloss: 0.117546466412\n",
      "\tacc: 0.950495049505\n",
      "\tauc: 0.979254368811\n",
      "\tap@k: 0.966324939367\n",
      "Train:\n",
      "\tloss: 0.137666410255\n",
      "\tacc: 0.941287128713\n",
      "\tauc: 0.956659230888\n",
      "\tap@k: 0.899420009154\n",
      "Val:\n",
      "\tloss: 0.145322262173\n",
      "\tacc: 0.953267326733\n",
      "\tauc: 0.975080516695\n",
      "\tap@k: 0.976437055258\n",
      "Train:\n",
      "\tloss: 0.133992971636\n",
      "\tacc: 0.945940594059\n",
      "\tauc: 0.963349619864\n",
      "\tap@k: 0.91102705361\n",
      "Val:\n",
      "\tloss: 0.139072462651\n",
      "\tacc: 0.957227722772\n",
      "\tauc: 0.981308601237\n",
      "\tap@k: 0.978199469255\n",
      "Train:\n",
      "\tloss: 0.138453378778\n",
      "\tacc: 0.948613861386\n",
      "\tauc: 0.962430402979\n",
      "\tap@k: 0.902870358996\n",
      "Val:\n",
      "\tloss: 0.147480364731\n",
      "\tacc: 0.940198019802\n",
      "\tauc: 0.970042379907\n",
      "\tap@k: 0.970279402779\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 100\n",
    "minibatches_per_epoch = 100\n",
    "# nontext_tr = nontext_tr.values\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    #training\n",
    "    epoch_y_true = []\n",
    "    epoch_y_pred = []\n",
    "    \n",
    "    b_c = b_loss = 0\n",
    "    for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "        iterate_minibatches(desc_tr,title_tr,nontext_tr,target_tr,batchsize=batch_size,shuffle=True)):\n",
    "        if j > minibatches_per_epoch:break\n",
    "            \n",
    "        loss,pred_probas = train_fun(b_desc,b_title,b_cat,b_y)\n",
    "        \n",
    "        b_loss += loss\n",
    "        b_c +=1\n",
    "        \n",
    "        epoch_y_true.append(b_y)\n",
    "        epoch_y_pred.append(pred_probas)\n",
    "    \n",
    "    epoch_y_true = np.concatenate(epoch_y_true)\n",
    "    epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "    \n",
    "    print(\"Train:\")\n",
    "    print('\\tloss:',b_loss/b_c)\n",
    "    print('\\tacc:',accuracy_score(epoch_y_true,epoch_y_pred>0.))\n",
    "    print('\\tauc:',roc_auc_score(epoch_y_true,epoch_y_pred))\n",
    "    print('\\tap@k:',APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1))\n",
    "    \n",
    "    #evaluation\n",
    "    epoch_y_true = []\n",
    "    epoch_y_pred = []\n",
    "    b_c = b_loss = 0\n",
    "    for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "        iterate_minibatches(desc_ts,title_ts,nontext_tr,target_ts,batchsize=batch_size,shuffle=True)):\n",
    "        if j > minibatches_per_epoch: break\n",
    "        loss,pred_probas = eval_fun(b_desc,b_title,b_cat,b_y)\n",
    "        \n",
    "        b_loss += loss\n",
    "        b_c +=1\n",
    "        \n",
    "        epoch_y_true.append(b_y)\n",
    "        epoch_y_pred.append(pred_probas)\n",
    "\n",
    "    epoch_y_true = np.concatenate(epoch_y_true)\n",
    "    epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "    \n",
    "    print(\"Val:\")\n",
    "    print('\\tloss:',b_loss/b_c)\n",
    "    print('\\tacc:',accuracy_score(epoch_y_true,epoch_y_pred>0.))\n",
    "    print('\\tauc:',roc_auc_score(epoch_y_true,epoch_y_pred))\n",
    "    print('\\tap@k:',APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"If you are seeing this, it's time to backup your notebook. No, really, 'tis too easy to mess up everything without noticing. \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final evaluation\n",
    "Evaluate network over the entire test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:\n",
      "\tloss: 0.135597576343\n",
      "\tacc: 0.94304\n",
      "\tauc: 0.969264801565\n",
      "\tap@k: 0.945693603199\n",
      "\n",
      "AUC:\n",
      "\tСойдёт, хотя можно ещё поднажать (ok)\n",
      "\n",
      "Accuracy:\n",
      "\tВсё ок (ok)\n",
      "\n",
      "Average precision at K:\n",
      "\tВы побили baseline (ok)\n"
     ]
    }
   ],
   "source": [
    "#evaluation\n",
    "epoch_y_true = []\n",
    "epoch_y_pred = []\n",
    "\n",
    "b_c = b_loss = 0\n",
    "nontext_ts = nontext_ts.values\n",
    "for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "    iterate_minibatches(desc_ts,title_ts,nontext_ts,target_ts,batchsize=batch_size,shuffle=True)):\n",
    "    loss,pred_probas = eval_fun(b_desc,b_title,b_cat,b_y)\n",
    "\n",
    "    b_loss += loss\n",
    "    b_c +=1\n",
    "\n",
    "    epoch_y_true.append(b_y)\n",
    "    epoch_y_pred.append(pred_probas)\n",
    "\n",
    "\n",
    "epoch_y_true = np.concatenate(epoch_y_true)\n",
    "epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "\n",
    "final_accuracy = accuracy_score(epoch_y_true,epoch_y_pred>0)\n",
    "final_auc = roc_auc_score(epoch_y_true,epoch_y_pred)\n",
    "final_apatk = APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1)\n",
    "\n",
    "print(\"Scores:\")\n",
    "print('\\tloss:',b_loss/b_c)\n",
    "print('\\tacc:',final_accuracy)\n",
    "print('\\tauc:',final_auc)\n",
    "print('\\tap@k:',final_apatk)\n",
    "score(final_accuracy,final_auc,final_apatk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main task\n",
    "\n",
    "* https://goo.gl/forms/eJwIeAbjxzVuo6vn1\n",
    "* Feel like Le'Cun:\n",
    " * accuracy > 0.95\n",
    " * AUC > 0.97\n",
    " * Average Precision at (test sample size * 0.025) > 0.99\n",
    " * And perhaps even farther\n",
    "\n",
    "* Casual mode\n",
    " * accuracy > 0.90\n",
    " * AUC > 0.95\n",
    " * Average Precision at (test sample size * 0.025) > 0.92\n",
    "\n",
    "* Remember the training, Luke\n",
    " * Dropout, regularization\n",
    " * Mommentum, RMSprop, ada*\n",
    " * etc etc etc\n",
    " \n",
    " * If you have background in texts, there may be a way to improve tokenizer, add some lemmatization, etc etc.\n",
    " * In case you know how not to shoot yourself in the foot with RNNs, they too may be of some use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
